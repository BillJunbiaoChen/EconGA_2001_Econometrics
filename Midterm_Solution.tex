\documentclass[11pt]{article}

%%METADATA
\title{GA2001 Econometrics \\Midterm Solution}
\author{
Junbiao Chen\thanks{E-mail: jc14076@nyu.edu.}
}
\date{\today}


%%PACKAGES
\usepackage{mdframed} % For boxed environments
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[hyphens]{url}
\usepackage{natbib}
\usepackage[font=normalsize,labelfont=bf]{caption}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue,citecolor=blue}
\usepackage{stmaryrd}  %Package with \boxast command
\usepackage{enumerate}% http://ctan.org/pkg/enumerate %Supports lowercase Roman-letter enumeration
\usepackage{verbatim} %Package with \begin{comment} environment
%\usepackage{enumitem}
\usepackage{physics}
\usepackage{tikz}
\usepackage{listings}
\usepackage{upquote}
\usepackage{booktabs} %Package with \toprule and \bottomrule
\usepackage{etoc}     %Package with \localtableofcontents
\usepackage{placeins}    %Package that prevent repositioning the tables
\usepackage{multicol}
\usepackage{bm}
\usepackage{subfig}
\usepackage{csquotes}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{language=bash,
  frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=false,
  tabsize=3
}

\lstset{language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=false,
  tabsize=4
}

\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9} %

%CUSTOM DEFINITIONS
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\setcounter{secnumdepth}{3}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{automata, positioning, arrows, calc}

\tikzset{
	->,  % makes the edges directed
	>=stealth, % makes the arrow heads bold
	shorten >=2pt, shorten <=2pt, % shorten the arrow
	node distance=3cm, % specifies the minimum distance between two nodes. Change if n
	every state/.style={draw=blue!55,very thick,fill=blue!20}, % sets the properties for each ’state’ n
	initial text=$ $, % sets the text that appears on the start arrow
 }

%% PROPOSITION
% Define the Proposition environment
\newmdenv[
  innerleftmargin=10pt, 
  innerrightmargin=10pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  linecolor=black, 
  linewidth=1pt,
  backgroundcolor=white, 
  roundcorner=5pt
]{propositionbox}

\newtheoremstyle{boldtitle} % Define a new theorem style
  {10pt} % Space above
  {10pt} % Space below
  {\itshape} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  { } % Space after theorem head
  {} % Theorem head spec

\theoremstyle{boldtitle} % Use the custom style
\newtheorem{proposition}{Proposition} % Define the proposition environment

% Redefine the proposition environment to use the box
\newenvironment{boxedproposition}[1][]
{\begin{propositionbox}\begin{proposition}[#1]}
{\end{proposition}\end{propositionbox}}

%%FORMATTING
\usepackage[bottom]{footmisc}
\onehalfspacing
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\bibliographystyle{../bib/aeanobold-oxford}


%main text
\begin{document}
\maketitle

\section{Problem I.}
\subsection{(a) Answer}
$\mathcal{F}_0^W = \{\emptyset, \Omega \}$,
where $\Omega = \{-1, +1\}^\mathbb{N}$.
(\textit{Remark:} Since the building blocks of simple random walks are incremental steps, 
we define an outcome $\omega \in \Omega$ as an infinite sequence of $-1$ and $+1$.)

\vspace{10mm}

\noindent \textbf{(i) Answer}
Define $A_1(+1) = \{\omega \in \Omega: X_1 = +1\}$ and
$A_1(-1) = \{\omega \in \Omega: X_1 = -1\}$.
then, we have 
\begin{align*}
\mathcal{F}_1^W & = \{\emptyset, A_1(+1), A_1(-1), A_1(+1) \cup A_1(-1),\Omega \} \\ 
& = \{\emptyset, A_1(+1), \Omega \backslash A_1(+1), \Omega \}
\end{align*}

Similarly, for $t = 2$, 
define 
\begin{align}
  & A_2(2) = \{\omega \in \Omega: (X_1(\omega), X_2(\omega)) = (+1, +1)  \} \\
  & A_2(-2) = \{\omega \in \Omega: (X_1(\omega), X_2(\omega)) = (-1, -1)  \}
\end{align}
\begin{equation}
  \begin{aligned}
  A_2(0) & = \{\omega \in \Omega: (X_1(\omega), X_2(\omega)) = (+1, -1) \cup  (-1, +1)  \} \\ 
    & = \{(+1, -1), (-1, +1) \} \times \{+1, -1\} \times ... ,
\end{aligned}
\end{equation}

then we have 
\begin{align*}
\mathcal{F}_2^W = \{& \emptyset, \Omega, \\ 
  & A_2(2), A_2(-2), A_2(0), \\
  & A_2(2) \cup A_2(-2), \\
  & A_2(2) \cup A_2(0),  \\ 
  & A_2(-2) \cup A_2(0) \}
\end{align*}
Therefore, $\mathcal{F}_1^W \neq \mathcal{F}_2^W$.

\vspace{5mm}
\noindent \textbf{Comment on property (i):}
In general, more information are revealed as time goes by. 
Hence, $\mathcal{F}_t^W \subset \mathcal{F}_{t+j}^W$ for any $j \geq 1$.

% Furthermore, the probability measure becomes more granular as time goes by.
% Concretely, at $t = 1$, we have $P(A_1(+1)) = P(A_1(-1)) = 1/2$;
% at $t = 2$, we have $P(A_2(2)) = P(A_2(-2)) = 1/4$ and $P(A_2(0)) = 1/2$.

\vspace{10mm}
\noindent \textbf{(ii) Answer} 
Under the $\sigma$-algebra $\mathcal{F}_{1,2}^X$, we can distinguish $\tilde{X}^1(1,-1) = (1, -1) \times \{1, -1\} \times ...$ from 
$\tilde{X}^1(-1,1) = (-1, 1) \times \{1, -1\} \times ...$, which are mingled in $A_2(0)$ defined in \textbf{(i)} above.
That is, $\tilde{X}^1(1,-1) \cup \tilde{X}^1(-1,1) = A_2(0)$.
Therefore, we have 
\begin{align*}
\mathcal{F}_{1,2}^X & = \sigma(A_2(2), A_2(-2), \tilde{X}^1(1,-1), \tilde{X}^1(-1,1)) \\ 
& = \{ \emptyset, \Omega,  A_2(2), A_2(-2), \tilde{X}^1(1,-1), \tilde{X}^1(-1,1)  \\ 
& \qquad A_2(2) \cup A_2(-2), \\ 
& \qquad A_2(2) \cup \tilde{X}^1(1,-1), \\
& \qquad A_2(-2) \cup \tilde{X}^1(1,-1), \\
& \qquad A_2(2) \cup \tilde{X}^1(-1,1), \\
& \qquad A_2(-2) \cup \tilde{X}^1(-1,1), \\
& \qquad\tilde{X}^1(-1,1) \cup \tilde{X}^1(1,-1)  \}\\ 
& \supset \mathcal{F}_{2}^W
\end{align*}

\vspace{5mm}
\noindent \textbf{Comment on property (ii):}
In general, $F_{1, 2, \dots, t}^X \supset F_{t}^W $.
This holds because $W_t = \sum_{i=1}^t X_i$ and more ``micro'' (more disaggregate) 
random variable includes more information than 
the one that is more ``macro'' (more aggregate).

\subsection{(b) Answer}
The hint states that 
``If two random vectors are in a one-to-one (measurable) relationship, then their generated $\sigma$-algebras are equal''.
Notice that 
$X_t = W_{t} - W_{t-1}$, we have 
$(W_0, W_1, \dots, W_t)$ one-to-one maps to $(X_1, \dots, X_t )$. 
Concretely, 
\begin{align*}
  (X_1, X_2, \dots, X_t) & = (W_1 - W_0, W_2 - W_1, \dots, W_t - W_{t-1}) \\
  (W_0, W_1, \dots, W_t) & = (X_0, X_1 + X_0, \dots, \sum_{i=1}^t X_i )
\end{align*}
Therefore, their $\sigma$-algebras are the equal, $\mathcal{F}_{1,\dots,t}^X = \mathcal{F}_{0,1,\dots,t}^W$.

Consider a realization of $(W_0, W_1, \dots, W_{t-1}) = (w_0, w_1, \dots, w_{t-1})$.
Let $W_t = w$ be an event and $(W_0, W_1, \dots, W_{t-1}) = (w_0, w_1, \dots, w_{t-1})$ be another event.
Then, we have
\begin{equation}
  \label{eqn1:random_walk_independence}
  \begin{aligned}
  \text{Prob}(W_t = w , (w_0, \dots, w_{t-2}) |W_{t-1}) & = \text{Prob}(W_t = w | (w_0, \dots, w_{t-2}), W_{t-1}) \times \text{Prob}((w_0, \dots, w_{t-2}) | W_{t-1}) \\
    & = \text{Prob}(W_t = w | W_{t-1}) \times \text{Prob}((w_0, w_1, \dots, w_{t-2}) | w_{t-1})
  \end{aligned}
\end{equation}
where I use the chain rule of conditional proability in the first equality.
In the second equality, I use the \textbf{Markov property} of random walks.

Therefore, by the definition of independence ($A \perp B \Leftrightarrow P(AB) = P(A) P(B)$),
we can conclude that 
\[
W_t \perp (W_0, W_1, \dots, W_{t-2}) | W_{t-1}
\]
\(\blacksquare\)


\subsection{(c) Answer}
\noindent \textbf{Part (i) Conditional Expectation} 
\\
\noindent By the definition of conditional expectation, we have
\begin{align*}
  \mathbb{E}(W_t | W_0, W_1, \dots, W_{t-1}) & = \sum_{w} w \times \text{Prob}(W_t = w | W_0, W_1, \dots, W_{t-1}) 
\end{align*}
Since random walks satify the Markov property, we have $\text{Prob}(W_t = w | W_0, \dots, W_{t-1}) = \text{Prob}(W_t = w |  W_{t-1})$.
Hence, we have 
\[
  \mathbb{E}(W_t | W_0, W_1, \dots, W_{t-1}) = \sum_{w} w \times \text{Prob}(W_t = w | W_{t-1})  = \mathbb{E}(W_t | W_{t-1})
\]
\(\blacksquare\)

\vspace{5mm}
For $\forall \omega \in \Omega$, we have
\[
\mathbb{E}(W_t | W_{t-1}) = \mathbb{E}(W_{t-1} + X_t | W_{t-1}) = W_{t-1},
\]
which demonstrates that the simple random walk satisfies the \textbf{martingale property}.

\vspace{5mm}
\noindent \textbf{Part (ii) Conditional Variance} 
\\
\noindent First, I prove that $\text{Var}(W_t | W_0, W_1, \dots, W_{t-1}) = \text{Var}(W_t | W_{t-1})$.
Following the logic in (i) above (of course, we invoke the Markov property here again), we have 
\begin{align}
\mathbb{E}(W_t^2 | W_0, W_1, \dots, W_{t-1}) = \mathbb{E}(W_t^2 | W_{t-1})
\end{align}
Therefore, we have 
\begin{align*}
\text{Var}(W_t | W_0, W_1, \dots, W_{t-1}) & = \mathbb{E}(W_t^2 | W_0, \dots, W_{t-1}) - \bigg[\mathbb{E}(W_t | W_0, \dots, W_{t-1})\bigg]^2 \\ 
& = \mathbb{E}(W_t^2 | W_{t-1}) - \bigg[\mathbb{E}(W_t | W_{t-1})\bigg]^2 \\
& = \text{Var}(W_t | W_{t-1})
\end{align*}
\(\blacksquare\)



\noindent Second, I compute the condition variance. For $\forall \omega \in \Omega$, we have
\begin{align*}
\text{Var}(W_t | W_{t-1}) & = \text{Var}(W_{t-1} + X_t | W_{t-1}) \\ 
& = \text{Var}(X_t | W_{t-1}) \\
& = \mathbb{E}(X_t^2 | W_{t-1}) - (\mathbb{E}(X_t | W_{t-1}))^2 \\  
& = \mathbb{E}(X_t^2 | W_{t-1}) \\ 
& = \mathbb{E}(X_t^2) \\ 
& = 1.
\end{align*}


\subsection{(d) Answer}
\noindent \textbf{Part (i) Ergodicity of $X_t$.}
\begin{align*}
  \lim_{n \rightarrow \infty} \mathbb{E} \bigg[\bigg(\frac{1}{n} \sum_{t=1}^{n} X_t - \frac{1}{n} \mathbb{E}(X_t) \bigg)^2 \bigg] 
  & =  \lim_{n \rightarrow \infty}  \frac{1}{n^2} \mathbb{E} \bigg[\bigg(\sum_{t=1}^{n} X_t \bigg)^2 \bigg] \\
  & = \lim_{n \rightarrow \infty}  \frac{1}{n^2}  \bigg(\sum_{i=1}^{n} \mathbb{E} [X_i^2] \bigg) \\
  & =  \lim_{n \rightarrow \infty} \frac{1}{n} = 0
\end{align*}
\noindent Therefore, $X_t$ is ergodic. \(\blacksquare\)

\vspace{5mm}
\noindent \textbf{Part (ii) Ergodicity of $W_t$.}
Let's derive some useful properties of $W_t$ first.
\begin{align}
\mathbb{E}(W_t) & = \mathbb{E}\left(\sum_{i=1}^t X_i \right) = \sum_{i=1}^t \mathbb{E}(X_i) = 0, \\
\mathbb{E}(W_t^2) & = \mathbb{E}\left(\sum_{i=1}^t X_i^2 \right) = \sum_{i=1}^t \mathbb{E}(X^2) = t, \\
\mathbb{E}(W_i W_j) & = \mathbb{E}\bigg[\left(\sum_{\ell=1}^i X_\ell  \right) \left(\sum_{k=1}^j X_k  \right) \bigg] = \min \{ i, j \}, \\
\end{align}
\noindent \textit{Remark:} To derive the third property, I use the fact that $\mathbb{E}(X_i X_j) = 0$ for $i \neq j$ 
and $\mathbb{E}(X_i^2) = 1$.

\noindent With these properties, we can compute the following.
\begin{align*}
  \lim_{n \rightarrow \infty} \mathbb{E} \bigg[\bigg(\frac{1}{n} \sum_{t=1}^{n} W_t - \frac{1}{n} \mathbb{E}(W_t) \bigg)^2 \bigg] 
  & =  \lim_{n \rightarrow \infty}  \frac{1}{n^2} \mathbb{E} \bigg[\bigg(\sum_{t=1}^{n} W_t \bigg)^2 \bigg] \\
  & = \lim_{n \rightarrow \infty}  \frac{1}{n^2}  \bigg(\sum_{i=1}^{n} \mathbb{E} [W_i^2] + 2 \sum_{j=i+1}^n \sum_{i=1}^{n-1} \mathbb{E}[W_i W_j] \bigg) \\
  & =  \lim_{n \rightarrow \infty} \frac{1}{n^2}  \bigg(\frac{n(n+1)}{2} + \frac{n^3 - n}{3} \bigg) \\
  & =  \lim_{n \rightarrow \infty} \frac{2n^2 + 3n + 1}{n}  > 0 \\
\end{align*}
\noindent Therefore, $W_t$ is not ergodic. \(\blacksquare\)

\section*{Problem II.}
\subsection{(a) Answer}
\textbf{(i) Why I think $\mathbb{E}[X\epsilon]\neq 0$:}

Consider a setting where $Y$ is savings, $X$ is participation in a retirement plan with 
other individual covariates.
$\mathbb{E}[X\epsilon] \neq 0$ because there might exist an unobserved factor that \textbf{jointly influences}
both savings and the retirement plan participation.
For example, \textbf{the frequency and amount of monetary transfers from family members} is an unobserved object 
(namely, a component of $\epsilon$), which affects retirement plan participation.
Concretely, individuals whose children are high-income earners might 
receive regular and more financial support from their children and thus save less and are less likely to participate in a retirement plan.

\vspace{5mm}
\noindent \textbf{(ii) The weight matrix of the optimal GMM estimator:}

The weighting matrix $W^*$ of the optimal GMM estimator $\hat{\beta}^*_{\text{GMM}}$ is given by 
\[
W^* = \Omega^{-1}.
\]

\vspace{5mm}
\noindent \textbf{(iii) The asymptotic variance of $\sqrt{n} \left( \hat{\beta}_{\text{GMM}} - \beta_0 \right)$:}

Plugging in the generic formula of the asymptotic variance of the GMM estimator,
the asymptotic variance is given by 
\begin{align*}
    (G'WG)^{-1}G'W\Omega W G(G'WG)^{-1} 
    & = (G'\Omega^{-1}G)^{-1}G'\Omega^{-1}\Omega \Omega^{-1} G(G'\Omega^{-1}G)^{-1} \\
    & = (G'\Omega^{-1}G)^{-1} \\
    & = \bigg[G'\bigg(\sigma_0^2(Z) \mathbb{E}(ZZ') \bigg)^{-1}G\bigg]^{-1} \\ 
    & = \sigma_0^2(Z) \bigg[G'\mathbb{E}(ZZ')^{-1}G\bigg]^{-1}
\end{align*}
where $\Omega := \mathbb{E}[ZZ'\epsilon^2] = \mathbb{E}[ZZ'\mathbb{E}[\epsilon^2|Z]] = \sigma_0^2(Z)  \mathbb{E}(ZZ')$ is given by the assumption. $G = \mathbb{E}[ZX']$.



\subsection{(b) Answer}
Consistent estimators of $W^*$ and $\Sigma^*$ is given by 


\subsection{(c) Answer}
Given the orthogonality condition $\mathbb{E}[Z\epsilon] = 0$, we can formulate the sample moment condition as
\[
\hat{g}(\beta) = \frac{1}{n} \sum_i g_i(\beta) = \frac{1}{n} \sum_i Z_i (Y_i - X_i'\beta).
\]
The first order condition for the minimization of $\hat{g}(\beta)' W \hat{g}(\beta)$ is given by
\[
0 = X'Z W Z'(y - X\beta) = X'Z W Z' y - X'Z W Z' X \beta.
\]
It follows that 
\[
\hat{\beta} = \left( X'Z W Z' X \right)^{-1} X'Z W Z' y.
\]
Let $W = (Z'Z)^{-1}$, then we have
\begin{align*}
\hat{\beta} & = \left( X'Z (Z'Z)^{-1} Z' X \right)^{-1} X'Z (Z'Z)^{-1} Z' y \\ 
    & = \left( X' P_Z X \right)^{-1} X' P_Z y,
\end{align*}
where $P_Z = Z(Z'Z)^{-1}Z'$ is the projection matrix onto the column space of $Z$.

\vspace{10mm}
\noindent \textbf{GMM and 2SLS IV:}

The following shows that $\hat{\beta}_{\text{GMM}}^* = \left( X'Z (Z'Z)^{-1} Z' X \right)^{-1} X'Z (Z'Z)^{-1} Z' Y$ can be derived from two-stage least squares.
In the first stage, we regress $X$ on $Z$, and obtain the predicted value $\hat{X}$:
\[
\hat{X} = Z \hat{\Gamma} = Z(Z'Z)^{-1}Z'X
\]
In the second stage, we regress $Y$ on $\hat{X}$ and obtain the 2SLS estimator:%
\footnote{
Note that 
\[
\hat{X}' = (Z(Z'Z)^{-1}Z'X)' = X'Z(Z'Z)^{-1}Z',
\]
and
\[
\hat{X}' \hat{X} = X'Z(Z'Z)^{-1}Z'Z(Z'Z)^{-1}Z'X = X'Z(Z'Z)^{-1}Z'X
\].
}
\begin{align*}
\hat{\beta}_{\text{2SLS}} & = (\hat{X}'\hat{X})^{-1}\hat{X}'Y \\ 
& = (X'Z(Z'Z)^{-1}Z'X)^{-1} X'Z(Z'Z)^{-1}Z' Y \\ 
& = \hat{\beta}_{\text{GMM}}^*.
\end{align*}

\noindent \textbf{The S.E. of the Optimal GMM and 2SLS:} 
Note that the weighting matrix in the 2SLS is $(\bm{Z}'\bm{Z})^{-1}$ (or equivalently, $n^{-1}(\bm{Z}'\bm{Z})^{-1}$), 
which asymptotically approaches to $(\mathbb{E}[ZZ'])^{-1}$. 
Namely, $n^{-1}(\bm{Z}'\bm{Z})^{-1} \rightarrow (\mathbb{E}[ZZ'])^{-1}$.
Meanwhile, the optimal weighting matrix is given by $\Omega^{-1} = \bigg[\mathbb{E}[ZZ' \mathbb{E}[\epsilon^2|Z] ] \bigg]^{-1}$.
Therefore, we can conclude that 
\begin{itemize}
  \item If we assume conditional \textbf{homoskedasticity} ($\mathbb{E}[\epsilon^2|Z] = \sigma^2 $), then the 2SLS can produce correct SE for $\hat{\beta}_{\text{GMM}}^*$.
  \item However, if the error are conditionally \textbf{heteroskedastic}, then the 2SLS is less efficient than the GMM. Hence, the SE from 2SLS is incorrect for $\hat{\beta}_{\text{GMM}}^*$.
\end{itemize}


\subsection{(d) Answer}

Define the regression error in the augmented regression as 
\[
\eta = Y - \mathbb{E}[Y|X, Z]
\]
Since $\mathbb{E}[\epsilon | Z, X] = V' \lambda_0$, we have 
\begin{align*}
  Y & = \mathbb{E}[Y|X, Z] + \eta \\
    & = \mathbb{E}[X'\beta_0 + \epsilon |X, Z] + \eta \\
    & = X'\beta_0 + V' \lambda_0 + \eta
\end{align*}


Replacing $V'$ with $\hat{V} = X - Z(Z'Z)^{-1} Z' X$, we have (expressing with observations in matrix form)
\[
Y =  X\beta_0 + \underbrace{(X - Z(Z'Z)^{-1} Z' X)}_{\equiv \hat{V}} \lambda_0 + \eta
\]

\paragraph{Equivalence between \textit{augmented}-$\hat{\beta}_{\text{OLS}}$ and $\hat{\beta}_{\text{GMM}}^*$.} 
In this subsection, I use the orthogonality condition $\mathbb{E}[ZV] = 0$ and the Frisch-Waugh-Lovell theorem to show that
the OLS estimator of $\beta_0$ in the augmented regression is equal to the optimal GMM estimator $\hat{\beta}_{\text{GMM}}^*$.

First, I derive a helpful property of matrix projection. 
Let $\hat{V} = X - P_Z X$, where $P_Z$ is a projection matrix.
Then, we have 
\begin{align*}
P_{\hat{V}} X & = P_{\hat{V}} (\hat{V} + P_Z X) \\ 
  & = P_{\hat{V}} \hat{V} + P_{\hat{V}} P_Z X \\ 
  & = \hat{V} + 0 \\ 
  & = X - P_Z X
\end{align*}
It follows that
\[
P_{\hat{V}} = I - P_Z.
\]

Next, I invoke the Frisch-Waugh-Lovell theorem to obtain the OLS estimate for $\beta_0$.
Recall that, for $Y = X_1 \beta_1 + X_2 \beta_2 + e$,
the Frisch-Waugh-Lovell theorem states that $\hat{\beta}_1 = (X_1' (I - P_{X_2}) X_1)^{-1} X_1' (I - P_{X_2}) Y$.
Therefore, we have 
\begin{align*}
\hat{\beta}_{\text{OLS}} & = (X' (I - P_{\hat{V}}) X)^{-1} X' (I - P_{\hat{V}}) Y \\ 
& = (X' P_Z X)^{-1} X' P_Z  Y \\
& = \left(X' Z (Z'Z)^{-1} Z' X \right)^{-1} X' Z (Z'Z)^{-1} Z' Y  \\
& = \hat{\beta}_{\text{GMM}}^*.
\end{align*}
\(\blacksquare\)


\subsection{(e) Answer}
From part (d) above, we have

\begin{align*}
\mathbb{E}\bigg[\sqrt{n} \left(\hat{\beta}_{\text{OLS}} - \beta_0\right) \bigg] & = \sqrt{n}\mathbb{E}\bigg[\left(X' P_Z X \right)^{-1} X' P_Z Y \bigg] - \sqrt{n}\beta_0 \\
& = \sqrt{n}\mathbb{E}\bigg[\left(X' P_Z X \right)^{-1} X' P_Z (X\beta_0 + \epsilon) \bigg] - \sqrt{n}\beta_0 \\ 
& = \sqrt{n} \mathbb{E} [\beta_0] + \sqrt{n} \mathbb{E} [\left(X' P_Z X \right)^{-1} X' Z (Z'Z)^{-1} Z'\epsilon] - \sqrt{n}\beta_0 \\ 
& = 0
\end{align*}

Futhermore, we can simplify $\hat{\beta}_{\text{OLS}} - \beta_0$ as follows.
\begin{align*}
\hat{\beta}_{\text{OLS}} - \beta_0 & = \left(X' P_Z X \right)^{-1} X' P_Z (X\beta_0 + V\lambda_0 + \eta) - \beta_0 \\
& = \beta_0 + \left(X' P_Z X \right)^{-1} X' P_Z V\lambda_0 + \left(X' P_Z X \right)^{-1} X' P_Z \eta - \beta_0  \\
& = \beta_0 + 0 + \left(X' P_Z X \right)^{-1} X' P_Z \eta - \beta_0 \\ 
& = \left(X' P_Z X \right)^{-1} X' P_Z \eta,
\end{align*}
where the third equlity uses the orthogonality condition between $P_Z X$ and $V$.

To derive the asymptotic variance, let's first compute some useful statistics.
\begin{align*}
  \mathbb{E}[\eta | Z] & = \mathbb{E}\bigg[\epsilon - \mathbb{E}[\epsilon | X, Z] | Z \bigg] \\ 
  & = \mathbb{E}[ \epsilon | Z] - \mathbb{E}\bigg[ \mathbb{E}[\epsilon | X, Z] | Z \bigg] \\
  & = \mathbb{E}\bigg[ \mathbb{E}[\epsilon | X, Z] | Z \bigg] - \mathbb{E}\bigg[ \mathbb{E}[\epsilon | X, Z] | Z \bigg] \\
  & = 0,
\end{align*}
where the third equality applies the Law of Iterated Expectation.

\begin{align*}
  \text{Var}(\eta) & = \mathbb{E}\bigg[\text{Var}(\eta | Z) \bigg] + \text{Var} \bigg(  \mathbb{E}[\eta | Z] \bigg) \\
    & = \mathbb{E}\bigg[ \mathbb{E}(\eta^2 | Z) - (\mathbb{E}(\eta | Z))^2 \bigg] + \text{Var} \bigg(  \mathbb{E}[\eta | Z] \bigg) \\
    & = \mathbb{E}[\sigma^2_\eta - 0] + 0 \\
    & = \sigma^2_\eta,
\end{align*}
where the first equality is due to the Law of Total Variance.
The third equality uses the assumption that $\mathbb{E}(\eta^2 | Z) = \sigma^2_\eta$.

Therefore, we have%
\footnote{
I use the fact that $P_Z$ is idempotent and $\text{Var}(\eta) = \sigma^2_\eta$ in the third equality.
}
\begin{align*}
\text{Var}[\sqrt{n} (\hat{\beta}_{\text{OLS}} - \beta_0)] & = n \text{Var}[(X' P_Z X)^{-1}X' P_Z \eta] \\ 
& = n (X' P_Z X)^{-1}X' P_Z \text{Var}(\eta) P_Z X (X' P_Z X)^{-1} \\ 
& = n \sigma^2_\eta (X' P_Z X)^{-1}X' P_Z X (X' P_Z X)^{-1} \\ 
& = n \sigma^2_\eta (X' P_Z X)^{-1}
\end{align*}


%% =========== %% ========= %%
\bibliography{../bib/notes.bib}

\end{document}