\documentclass[11pt]{article}

%%METADATA
\title{GA2001 Econometrics \\Midterm Solution}
\author{
Junbiao Chen\thanks{E-mail: jc14076@nyu.edu.}
}
\date{\today}


%%PACKAGES
\usepackage{mdframed} % For boxed environments
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[hyphens]{url}
\usepackage{natbib}
\usepackage[font=normalsize,labelfont=bf]{caption}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue,citecolor=blue}
\usepackage{stmaryrd}  %Package with \boxast command
\usepackage{enumerate}% http://ctan.org/pkg/enumerate %Supports lowercase Roman-letter enumeration
\usepackage{verbatim} %Package with \begin{comment} environment
%\usepackage{enumitem}
\usepackage{physics}
\usepackage{tikz}
\usepackage{listings}
\usepackage{upquote}
\usepackage{booktabs} %Package with \toprule and \bottomrule
\usepackage{etoc}     %Package with \localtableofcontents
\usepackage{placeins}    %Package that prevent repositioning the tables
\usepackage{multicol}
\usepackage{bm}
\usepackage{subfig}
\usepackage{csquotes}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{language=bash,
  frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=false,
  tabsize=3
}

\lstset{language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=false,
  tabsize=4
}

\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9} %

%CUSTOM DEFINITIONS
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\setcounter{secnumdepth}{3}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{automata, positioning, arrows, calc}

\tikzset{
	->,  % makes the edges directed
	>=stealth, % makes the arrow heads bold
	shorten >=2pt, shorten <=2pt, % shorten the arrow
	node distance=3cm, % specifies the minimum distance between two nodes. Change if n
	every state/.style={draw=blue!55,very thick,fill=blue!20}, % sets the properties for each ’state’ n
	initial text=$ $, % sets the text that appears on the start arrow
 }

%% PROPOSITION
% Define the Proposition environment
\newmdenv[
  innerleftmargin=10pt, 
  innerrightmargin=10pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  linecolor=black, 
  linewidth=1pt,
  backgroundcolor=white, 
  roundcorner=5pt
]{propositionbox}

\newtheoremstyle{boldtitle} % Define a new theorem style
  {10pt} % Space above
  {10pt} % Space below
  {\itshape} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  { } % Space after theorem head
  {} % Theorem head spec

\theoremstyle{boldtitle} % Use the custom style
\newtheorem{proposition}{Proposition} % Define the proposition environment

% Redefine the proposition environment to use the box
\newenvironment{boxedproposition}[1][]
{\begin{propositionbox}\begin{proposition}[#1]}
{\end{proposition}\end{propositionbox}}

%%FORMATTING
\usepackage[bottom]{footmisc}
\onehalfspacing
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\bibliographystyle{../bib/aeanobold-oxford}


%main text
\begin{document}
\maketitle

\section{Problem I.}
\subsection{(a) Answer}
$\mathcal{F}_0^W = \{\emptyset, \Omega \}$,
where $\Omega = \{-1, +1\}^\mathbb{N}$.
(\textit{Remark:} Since the building blocks of simple random walks are incremental steps, 
we define an outcome $\omega \in \Omega$ as an infinite sequence of $-1$ and $+1$.)

\vspace{10mm}

\noindent \textbf{(i) Answer}

Define $A_t = \{\omega \in \Omega: X_t(\omega) = +1\}$, and $\tilde{A}_t = \Omega \backslash A_t$.
Then, we have 
\begin{align*}
\mathcal{F}_1^W & = \{\emptyset, A_1,  \tilde{A}_1, \Omega \}
\end{align*}


Similarly, for $t = 2$, 
define 
\begin{align}
  & \{W_2 = 2\} := \{\omega \in \Omega: (X_1(\omega), X_2(\omega)) = (+1, +1)  \}  = A_1 \cap A_2\\
  & \{W_2 = -2\} := \{\omega \in \Omega: (X_1(\omega), X_2(\omega)) = (-1, -1)  \} = \tilde{A}_1 \cap \tilde{A}_2
\end{align}
\begin{equation}
  \begin{aligned}
  \{W_2 = 0 \} & := \{\omega \in \Omega: (X_1(\omega), X_2(\omega)) = (+1, -1) \cup  (-1, +1)  \} \\ 
    & = \left(A_1 \cap \tilde{A}_2 \right) \bigcup \left(\tilde{A}_1 \cap A_2 \right),
\end{aligned}
\end{equation}

then we have 
\begin{align*}
\mathcal{F}_2^W = \{& \emptyset, \Omega, \\ 
  & \{W_2 = 2\}, \{W_2 = -2\}, \{W_2 = 0\}, \\
  & \{W_2 = 2\} \cup \{W_2 = -2\}, \\
  & \{W_2 = 2\} \cup \{W_2 = 0\},  \\ 
  & \{W_2 = -2\} \cup \{W_2 = 0\} \}
\end{align*}
Therefore, $\mathcal{F}_1^W \cap \mathcal{F}_2^W = \{\emptyset, \Omega, A_1, \tilde{A}_1 \}$.
Hence, $\mathcal{F}_1^W \neq \mathcal{F}_2^W$.

\vspace{5mm}
\noindent \textbf{A general statement for $t \geq 2$ on property (i):}
In general, more information are revealed as time goes by. 
Hence, $\mathcal{F}_t^W \neq \mathcal{F}_{t+j}^W$ for any $j \geq 1$.

% Furthermore, the probability measure becomes more granular as time goes by.
% Concretely, at $t = 1$, we have $P(A_1(+1)) = P(A_1(-1)) = 1/2$;
% at $t = 2$, we have $P(A_2(2)) = P(A_2(-2)) = 1/4$ and $P(A_2(0)) = 1/2$.

\vspace{10mm}
\noindent \textbf{(ii) Answer} 
Under the $\sigma$-algebra $\mathcal{F}_{1,2}^X$, we can distinguish the following two outcomes:
\begin{align*}
  \hat{X}(1,-1) & = (1, -1) \times \{1, -1\} \times ... \\
  \hat{X}(-1,1) & = (-1, 1) \times \{1, -1\} \times ...
\end{align*}
which are mingled in the set $\{W_2 = 0\}$ defined in \textbf{(i)} above.
In other words, $\hat{X}(1,-1) \cup \hat{X}(-1,1) = \{W_2 = 0\}$.
Therefore, we have 
\begin{align*}
\mathcal{F}_{1,2}^X & = \sigma \bigg(\{W_2 = 2\}, \{W_2 = -2\}, \hat{X}(1,-1), \hat{X}(-1,1) \bigg) \\ 
& = \bigg\{ \emptyset, \Omega,  \{W_2 = 2\}, \{W_2 = -2\}, \hat{X}(1,-1), \hat{X}(-1,1)  \\ 
& \qquad \{W_2 = 2\} \cup \{W_2 = -2\}, \\
& \qquad \{W_2 = 2\} \cup \hat{X}(1,-1), \\
& \qquad \{W_2 = -2\} \cup \hat{X}(1,-1), \\
& \qquad \{W_2 = 2\} \cup \hat{X}(-1,1), \\
& \qquad \{W_2 = -2\} \cup \hat{X}(-1,1), \\
& \qquad \hat{X}(-1,1) \cup \hat{X}(1,-1)  \bigg\}\\
& \supset \mathcal{F}_{2}^W
\end{align*}

\vspace{5mm}
\noindent \textbf{A general statement for $t \geq 2$ on property (ii):}
In general, $F_{1, 2, \dots, t}^X \supset F_{t}^W $.
This holds because $W_t = \sum_{i=1}^t X_i$ and more ``micro'' (more disaggregate) 
random variable includes more information than 
the one that is more ``macro'' (more aggregate).

\subsection{(b) Answer}
The hint states that 
``If two random vectors are in a one-to-one (measurable) relationship, then their generated $\sigma$-algebras are equal''.
Notice that 
$X_t = W_{t} - W_{t-1}$, we have 
$(W_0, W_1, \dots, W_t)$ one-to-one maps to $(X_1, \dots, X_t )$. 
Concretely, 
\begin{align*}
  (X_1, X_2, \dots, X_t) & = (W_1 - W_0, W_2 - W_1, \dots, W_t - W_{t-1}) \\
  (W_0, W_1, \dots, W_t) & = (X_0, X_1 + X_0, \dots, \sum_{i=1}^t X_i )
\end{align*}
Therefore, their $\sigma$-algebras are the equal, $\mathcal{F}_{1,\dots,t}^X = \mathcal{F}_{0,1,\dots,t}^W$.

Consider a realization of $(W_0, W_1, \dots, W_{t-1}) = (w_0, w_1, \dots, w_{t-1})$.
Let $W_t = w$ be an event and $(W_0, W_1, \dots, W_{t-1}) = (w_0, w_1, \dots, w_{t-1})$ be another event.
Then, we have
\begin{equation}
  \label{eqn1:random_walk_independence}
  \begin{aligned}
  \text{Prob}(W_t = w , (w_0, \dots, w_{t-2}) |W_{t-1}) & = \text{Prob}(W_t = w | (w_0, \dots, w_{t-2}), W_{t-1}) \times \text{Prob}((w_0, \dots, w_{t-2}) | W_{t-1}) \\
    & = \text{Prob}(W_t = w | W_{t-1}) \times \text{Prob}((w_0, w_1, \dots, w_{t-2}) | W_{t-1})
  \end{aligned}
\end{equation}
where I use the chain rule of conditional proability in the first equality.
In the second equality, I use the \textbf{Markov property} of random walks.

Therefore, by the definition of independence ($A \perp B \Leftrightarrow P(AB) = P(A) P(B)$),
we can conclude that 
\[
W_t \perp (W_0, W_1, \dots, W_{t-2}) | W_{t-1}
\]
\(\blacksquare\)


\subsection{(c) Answer}
\noindent \textbf{Part (i) Conditional Expectation} 
\\
\noindent By the definition of conditional expectation, we have
\begin{align*}
  \mathbb{E}(W_t | W_0, W_1, \dots, W_{t-1}) & = \sum_{w} w \times \text{Prob}(W_t = w | W_0, W_1, \dots, W_{t-1}) 
\end{align*}
Since random walks satify the Markov property, we have $\text{Prob}(W_t = w | W_0, \dots, W_{t-1}) = \text{Prob}(W_t = w |  W_{t-1})$.
Hence, we have 
\[
  \mathbb{E}(W_t | W_0, W_1, \dots, W_{t-1}) = \sum_{w} w \times \text{Prob}(W_t = w | W_{t-1})  = \mathbb{E}(W_t | W_{t-1})
\]
\(\blacksquare\)

\vspace{5mm}
For $\forall \omega \in \Omega$, we have
\[
\mathbb{E}(W_t | W_{t-1}) = \mathbb{E}(W_{t-1} + X_t | W_{t-1}) = W_{t-1},
\]
which demonstrates that the simple random walk satisfies the \textbf{martingale property}.

\vspace{5mm}
\noindent \textbf{Part (ii) Conditional Variance} 
\\
\noindent First, I prove that $\text{Var}(W_t | W_0, W_1, \dots, W_{t-1}) = \text{Var}(W_t | W_{t-1})$.
Following the logic in (i) above (of course, we invoke the Markov property here again), we have 
\begin{align}
\mathbb{E}(W_t^2 | W_0, W_1, \dots, W_{t-1}) = \mathbb{E}(W_t^2 | W_{t-1})
\end{align}
Therefore, we have 
\begin{align*}
\text{Var}(W_t | W_0, W_1, \dots, W_{t-1}) & = \mathbb{E}(W_t^2 | W_0, \dots, W_{t-1}) - \bigg[\mathbb{E}(W_t | W_0, \dots, W_{t-1})\bigg]^2 \\ 
& = \mathbb{E}(W_t^2 | W_{t-1}) - \bigg[\mathbb{E}(W_t | W_{t-1})\bigg]^2 \\
& = \text{Var}(W_t | W_{t-1})
\end{align*}
\(\blacksquare\)



\noindent Second, I compute the condition variance. For $\forall \omega \in \Omega$, we have
\begin{align*}
\text{Var}(W_t | W_{t-1}) & = \text{Var}(W_{t-1} + X_t | W_{t-1}) \\ 
& = \text{Var}(X_t | W_{t-1}) \\
& = \mathbb{E}(X_t^2 | W_{t-1}) - (\mathbb{E}(X_t | W_{t-1}))^2 \\  
& = \mathbb{E}(X_t^2 | W_{t-1}) \\ 
& = \mathbb{E}(X_t^2) \\ 
& = 1.
\end{align*}


\subsection{(d) Answer}
\noindent \textbf{Part (i) Ergodicity of $X_t$.}
\begin{align*}
  \lim_{n \rightarrow \infty} \mathbb{E} \bigg[\bigg(\frac{1}{n} \sum_{t=1}^{n} X_t - \frac{1}{n} \mathbb{E}(X_t) \bigg)^2 \bigg] 
  & =  \lim_{n \rightarrow \infty}  \frac{1}{n^2} \mathbb{E} \bigg[\bigg(\sum_{t=1}^{n} X_t \bigg)^2 \bigg] \\
  & = \lim_{n \rightarrow \infty}  \frac{1}{n^2}  \bigg(\sum_{i=1}^{n} \mathbb{E} [X_i^2] \bigg) \\
  & =  \lim_{n \rightarrow \infty} \frac{1}{n} = 0
\end{align*}
\noindent Therefore, $X_t$ is ergodic in mean. \(\blacksquare\)

\vspace{5mm}
\noindent \textbf{Part (ii) Ergodicity of $W_t$.}
Let's derive some useful properties of $W_t$ first.
\begin{align}
\mathbb{E}(W_t) & = \mathbb{E}\left(\sum_{i=1}^t X_i \right) = \sum_{i=1}^t \mathbb{E}(X_i) = 0, \\
\mathbb{E}(W_t^2) & = \mathbb{E}\left(\sum_{i=1}^t X_i^2 \right) = \sum_{i=1}^t \mathbb{E}(X^2) = t, \\
\mathbb{E}(W_i W_j) & = \mathbb{E}\bigg[\left(\sum_{\ell=1}^i X_\ell  \right) \left(\sum_{k=1}^j X_k  \right) \bigg] = \min \{ i, j \}, \\
\end{align}
\noindent \textit{Remark:} To derive the third property, I use the fact that $\mathbb{E}(X_i X_j) = 0$ for $i \neq j$ 
and $\mathbb{E}(X_i^2) = 1$.

\noindent With these properties, we can compute the following.
\begin{align*}
  \lim_{n \rightarrow \infty} \mathbb{E} \bigg[\bigg(\frac{1}{n} \sum_{t=1}^{n} W_t - \frac{1}{n} \mathbb{E}(W_t) \bigg)^2 \bigg] 
  & =  \lim_{n \rightarrow \infty}  \frac{1}{n^2} \mathbb{E} \bigg[\bigg(\sum_{t=1}^{n} W_t \bigg)^2 \bigg] \\
  & = \lim_{n \rightarrow \infty}  \frac{1}{n^2}  \bigg(\sum_{i=1}^{n} \mathbb{E} [W_i^2] + 2 \sum_{j=i+1}^n \sum_{i=1}^{n-1} \mathbb{E}[W_i W_j] \bigg) \\
  & =  \lim_{n \rightarrow \infty} \frac{1}{n^2}  \bigg(\frac{n(n+1)}{2} + \frac{n^3 - n}{3} \bigg) \\
  & =  \lim_{n \rightarrow \infty} \frac{2n^2 + 3n + 1}{n}  > 0 \\
\end{align*}
\noindent Therefore, $W_t$ is not ergodic in mean. \(\blacksquare\)

\section{Problem II.}
\subsection{(a) Answer}
\textbf{(i) Why I think $\mathbb{E}[X\epsilon]\neq 0$:}

Consider a setting where $Y$ is savings, $X$ is participation in a retirement plan with 
other individual covariates.
$\mathbb{E}[X\epsilon] \neq 0$ because there might exist unobserved factors that \textbf{jointly influence}
both savings and the retirement plan participation.
For example, \textbf{the frequency and amount of monetary transfers from family members} is an unobserved object 
(namely, a component of $\epsilon$), which affects retirement plan participation.
Concretely, individuals whose children are high-income earners might 
receive regular and more financial support from their children and thus intend to save less 
and are less likely to participate in a retirement plan.

\vspace{5mm}
\noindent \textbf{(ii) The weighting matrix of the optimal GMM estimator:}
The GMM estimator is given by 
\[
\hat{\beta}_{\text{GMM}} = \text{argmin}_{\beta}  \bigg[ (\bm{Y-X}\beta)'\bm{ZWZ'}(\bm{Y-X}\beta) \bigg]
\]
Taking derivative w.r.t. $\beta$, we have 
\[
\bm{X'ZWZ'(Y-X}\beta) = 0 \Rightarrow \hat{\beta}_{\text{GMM}} = \bm{(X'ZWZ'X)}^{-1} \bm{X'ZWZ'Y}
\]
Applying the Law of Large Number (LLN), Central Limit Theorem (CLT), and the Slutzky Theorem, the asymptotic distribution of 
$\sqrt{n}(\hat{\beta}_{\text{GMM}} - \beta_0)$ is given by 
\[
\sqrt{n}(\hat{\beta}_{\text{GMM}} - \beta_0) \rightarrow_d \mathcal{N}(\bm{0}, (G'WG)^{-1}G'W\Omega W G (G'WG)^{-1}),
\]
where $\Omega := \mathbb{E}[ZZ'\epsilon^2]$.
$G = \mathbb{E}[ZX']$.%
\footnote{
  $Z$ is a random vector whose dimension is $p \times 1$. 
  $X$ is a random vector whose dimension is $k \times 1$.
  Hence, dim($G$) = $p \times k$.
}


Following our lecture notes, 
the weighting matrix $W^*$ of the optimal GMM estimator is given by 
\[
W^* = \Omega^{-1},
\]

$W^* = \Omega^{-1}$ is the optimal (most efficient) weighting matrix because we can show that 
\[
(G'WG)^{-1}G'W\Omega W G (G'WG)^{-1} - (G'\Omega^{-1}G)^{-1} \text{ is a positive definite matrix}.
\]



\vspace{5mm}
\noindent \textbf{(iii) The asymptotic variance of $\sqrt{n} \left( \hat{\beta}_{\text{GMM}} - \beta_0 \right)$ with more structure:}

Plugging in the generic formula of the asymptotic variance of the GMM estimator,
the asymptotic variance is given by 
\begin{align*}
    (G'WG)^{-1}G'W\Omega W G(G'WG)^{-1} 
    & = (G'\Omega^{-1}G)^{-1}G'\Omega^{-1}\Omega \Omega^{-1} G(G'\Omega^{-1}G)^{-1} \\
    & = (G'\Omega^{-1}G)^{-1}
\end{align*}
Since $\Omega := \mathbb{E}[ZZ'\epsilon^2] = \mathbb{E}[ZZ'\mathbb{E}[\epsilon^2|Z]] = \mathbb{E}(ZZ'\sigma_0^2(Z))$ by the assumption. 
Plugging it into the equation above, we have the asymptotic variance given by 
\begin{align}
    (G'\Omega^{-1}G)^{-1}
    & = \bigg[\mathbb{E}[XZ'] \mathbb{E}(ZZ'\sigma_0^2(Z))^{-1} \mathbb{E}[ZX']\bigg]^{-1}
\end{align}

\subsection{(b) Answer}
Since we assume conditional homoskedasticity, $\mathbb{E}[\epsilon^2|Z] = \sigma_0^2$.
I propose consistent estimators of $W^*$ and $\Sigma^*$ as follows.%
\footnote{
  Under the assumption of conditional homoskedasticity, the generic form of 
  \begin{align*}
    \hat{W} & = \frac{1}{n} \sum_{i=1}^{n}
  \left(\frac{1}{n} \sum_{i=1}^{n} Z_i Z_i' (Y_i - X_i' \hat{\beta})^2 \right)^{-1}
  \end{align*} becomes 
  $\left(\frac{1}{n} \sum_{i=1}^{n} Z_i Z_i'\right)^{-1}$ because weighting matrix does not depend on scaling.
}
\begin{align}
  \hat{W} & := \left(\hat{\sigma}^2  \frac{1}{n} \sum_{i=1}^{n} Z_i Z_i' \right)^{-1} = (\hat{\sigma}^2 \bm{Z}'\bm{Z}/n)^{-1},
\end{align}
where $Z_i$ is $p\times 1$ vector (observation), and $\bm{Z} = (Z_1', Z_2', \dots, Z_n')'$ is an $n \times p$ matrix.
$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (Y_i - X_i' \hat{\beta})^2$.
With $\hat{W}$, we can construct the consistent estimator of the asymptotic variance as
\begin{align}
  \hat{\Sigma} & = \bigg[\hat{G}'\hat{W} \hat{G}\bigg]^{-1}
\end{align}
where $\hat{G} := \frac{1}{n} \sum_{i=1}^n Z_i X_i' = \bm{Z}'\bm{X}/n$.


\vspace{5mm}
\noindent \textbf{Justifications for my proposals:}
\begin{itemize}
  \item By LLN, $\hat{\sigma}^2 \rightarrow_p \sigma_0^2$ and $\bm{Z}'\bm{Z}/n \rightarrow_p \mathbb{E}(Z'Z).$ 
  Hence $\hat{W} \rightarrow_p W^*$.
  \item Again by LLN, $\hat{G} \rightarrow_p \mathbb{E}[ZX']$.
  \item In fact, the propose $\hat{W}$ and $\hat{G}$ above are the same as the ones used in 2SLS, which are efficient
  with conditional homoskedasticity,
\end{itemize}

\subsection{(c) Answer}
Given the orthogonality condition $\mathbb{E}[Z\epsilon] = 0$, we can formulate the sample moment condition as
\[
\hat{g}(\beta) = \frac{1}{n} \sum_i g_i(\beta) = \frac{1}{n} \sum_i Z_i (Y_i - X_i'\beta).
\]
The first order condition for the minimization of $\hat{g}(\beta)' W \hat{g}(\beta)$ is given by (in data matrix form)
\[
0 = X'Z W Z'(Y - X\beta) = X'Z W Z' Y - X'Z W Z' X \beta.
\]
It follows that 
\[
\hat{\beta} = \left( X'Z W Z' X \right)^{-1} X'Z W Z' Y.
\]
Let $W = (Z'Z)^{-1}$, then we have
\begin{align*}
\hat{\beta} & = \left( X'Z (Z'Z)^{-1} Z' X \right)^{-1} X'Z (Z'Z)^{-1} Z' Y \\ 
    & = \left( X' P_Z X \right)^{-1} X' P_Z Y,
\end{align*}
where $P_Z = Z(Z'Z)^{-1}Z'$ is the projection matrix.

\vspace{10mm}
\noindent \textbf{GMM and 2SLS IV:}

The following shows that $\hat{\beta}_{\text{GMM}}^* = \left( X'Z (Z'Z)^{-1} Z' X \right)^{-1} X'Z (Z'Z)^{-1} Z' Y$ can be derived from two-stage least squares.
In the first stage, we regress $X$ on $Z$, and obtain the predicted value $\hat{X}$:
\[
\hat{X} = Z \hat{\Gamma} = Z(Z'Z)^{-1}Z'X
\]
In the second stage, we regress $Y$ on $\hat{X}$ and obtain the 2SLS estimator:%
\footnote{
Note that 
\[
\hat{X}' = (Z(Z'Z)^{-1}Z'X)' = X'Z(Z'Z)^{-1}Z',
\]
and
\[
\hat{X}' \hat{X} = X'Z(Z'Z)^{-1}Z'Z(Z'Z)^{-1}Z'X = X'Z(Z'Z)^{-1}Z'X
\].
}
\begin{align*}
\hat{\beta}_{\text{2SLS}} & = (\hat{X}'\hat{X})^{-1}\hat{X}'Y \\ 
& = (X'Z(Z'Z)^{-1}Z'X)^{-1} X'Z(Z'Z)^{-1}Z' Y \\ 
& = \hat{\beta}_{\text{GMM}}^*.
\end{align*}

\noindent \textbf{The S.E. of the Optimal GMM and 2SLS:} 
Note that the weighting matrix in the 2SLS is $(\bm{Z}'\bm{Z})^{-1}$ (or equivalently, $n^{-1}(\bm{Z}'\bm{Z})^{-1}$), 
which asymptotically approaches to $(\mathbb{E}[ZZ'])^{-1}$. 
Namely, $n^{-1}(\bm{Z}'\bm{Z})^{-1} \rightarrow (\mathbb{E}[ZZ'])^{-1}$.
Meanwhile, the optimal weighting matrix is given by $\Omega^{-1} = \bigg[\mathbb{E}[ZZ' \mathbb{E}[\epsilon^2|Z] ] \bigg]^{-1}$.
Therefore, we can conclude that 
\begin{itemize}
  \item If we assume conditional \textbf{homoskedasticity} ($\mathbb{E}[\epsilon^2|Z] = \sigma^2 $), then the 2SLS can produce correct SE for $\hat{\beta}_{\text{GMM}}^*$.
  \item However, if the error are conditionally \textbf{heteroskedastic}, then the 2SLS is less efficient than the GMM. Hence, the SE from 2SLS is incorrect for $\hat{\beta}_{\text{GMM}}^*$.
\end{itemize}


\subsection{(d) Answer}

Define the regression error in the augmented regression as 
\[
\eta = Y - \mathbb{E}[Y|X, Z]
\]
Since $\mathbb{E}[\epsilon | Z, X] = V' \lambda_0$, we have 
\begin{align*}
  Y & = \mathbb{E}[Y|X, Z] + \eta \\
    & = \mathbb{E}[X'\beta_0 + \epsilon |X, Z] + \eta \\
    & = X'\beta_0 + V' \lambda_0 + \eta
\end{align*}


Replacing $V'$ with $\hat{V} = X - Z(Z'Z)^{-1} Z' X$, we have (expressing with observations in matrix form)
\[
Y =  X\beta_0 + \underbrace{(X - Z(Z'Z)^{-1} Z' X)}_{\equiv \hat{V}} \lambda_0 + \eta
\]

\paragraph{Equivalence between \textit{augmented}-$\hat{\beta}_{\text{OLS}}$ and $\hat{\beta}_{\text{GMM}}^*$.} 
In this subsection, I use the orthogonality condition $\mathbb{E}[ZV] = 0$ and the Frisch-Waugh-Lovell theorem to show that
the OLS estimator of $\beta_0$ in the augmented regression is equal to the optimal GMM estimator $\hat{\beta}_{\text{GMM}}^*$.

First, I derive a helpful property of matrix projection. 
Let $\hat{V} = X - P_Z X$, where $P_Z$ is a projection matrix.
Then, we have 
\begin{align*}
P_{\hat{V}} X & = P_{\hat{V}} (\hat{V} + P_Z X) \\ 
  & = P_{\hat{V}} \hat{V} + P_{\hat{V}} P_Z X \\ 
  & = \hat{V} + 0 \\ 
  & = X - P_Z X
\end{align*}
It follows that
\[
P_{\hat{V}} = I - P_Z.
\]

Next, I invoke the Frisch-Waugh-Lovell theorem to obtain the OLS estimate for $\beta_0$.
Recall that, for $Y = X_1 \beta_1 + X_2 \beta_2 + e$,
the Frisch-Waugh-Lovell theorem states that $\hat{\beta}_1 = (X_1' (I - P_{X_2}) X_1)^{-1} X_1' (I - P_{X_2}) Y$.
Therefore, we have 
\begin{align*}
\hat{\beta}_{\text{OLS}} & = (X' (I - P_{\hat{V}}) X)^{-1} X' (I - P_{\hat{V}}) Y \\ 
& = (X' P_Z X)^{-1} X' P_Z  Y \\
& = \left(X' Z (Z'Z)^{-1} Z' X \right)^{-1} X' Z (Z'Z)^{-1} Z' Y  \\
& = \hat{\beta}_{\text{GMM}}^*.
\end{align*}
\(\blacksquare\)


\subsection{(e) Answer}
From part (d) above, we can simplify $\hat{\beta}_{\text{OLS}} - \beta_0$ as follows.
\begin{align*}
\hat{\beta}_{\text{OLS}} - \beta_0 & = \left(\bm{X'P_Z X} \right)^{-1} \bm{X' P_Z} (\bm{X}\beta_0 + \bm{V}\lambda_0 + \eta) - \beta_0 \\
& = \beta_0 + \left(\bm{X'P_Z X} \right)^{-1} \bm{X' P_Z V}\lambda_0 + \left(\bm{X'P_Z X} \right)^{-1} \bm{X' P_Z} \eta - \beta_0  \\
& = \beta_0 + 0 + \left(\bm{X'P_Z X} \right)^{-1} \bm{X' P_Z} \eta - \beta_0 \\ 
& = \left(\bm{X'P_Z X} \right)^{-1} \bm{X' P_Z} \eta,
\end{align*}
where the third equlity uses the orthogonality condition between $P_Z X$ and $V$.

It follows that 
\begin{align*}
\sqrt{n} \left(\hat{\beta}_{\text{OLS}} - \beta_0 \right) & = 
 (n^{-1} \bm{X'P_Z X} )^{-1} \bigg(n^{-1/2}  \bm{X' P_Z} \eta\bigg)
\end{align*}

By the LLN, we have 
\[
n^{-1} \bm{X'P_Z X} = n^{-1} \bm{X}' \bm{Z(Z'Z)^{-1}Z' X} \rightarrow_p 
\mathbb{E} \left[ XZ' \right] \mathbb{E} \left[ ZZ' \right]^{-1} \mathbb{E} \left[ ZX' \right]
\]

By CLT, we have 
\[
n^{-1/2}  \bm{X' P_Z} \eta \rightarrow_d \mathcal{N}(0, \sigma_\eta^2 \mathbb{E} \left[ XZ' \right] \mathbb{E} \left[ ZZ' \right]^{-1} \mathbb{E} \left[ ZX' \right])
\]

Finally, by Slutzky theorem, we have 
\[
(n^{-1} \bm{X'P_Z X} )^{-1} \bigg(n^{-1/2}  \bm{X' P_Z} \eta\bigg) \rightarrow_d
\mathcal{N}\bigg\{0, \sigma_\eta^2 \bigg(\mathbb{E} \left[ XZ' \right] \mathbb{E} \left[ ZZ' \right]^{-1} \mathbb{E} \left[ ZX' \right]\bigg)^{-1} \bigg\}.
\]
% To derive the asymptotic variance, let's first compute some useful statistics.
% \begin{align*}
%   \mathbb{E}[\eta | Z] & = \mathbb{E}\bigg[\epsilon - \mathbb{E}[\epsilon | X, Z] | Z \bigg] \\ 
%   & = \mathbb{E}[ \epsilon | Z] - \mathbb{E}\bigg[ \mathbb{E}[\epsilon | X, Z] | Z \bigg] \\
%   & = \mathbb{E}\bigg[ \mathbb{E}[\epsilon | X, Z] | Z \bigg] - \mathbb{E}\bigg[ \mathbb{E}[\epsilon | X, Z] | Z \bigg] \\
%   & = 0,
% \end{align*}
% where the third equality applies the Law of Iterated Expectation.

% \begin{align*}
%   \text{Var}(\eta) & = \mathbb{E}\bigg[\text{Var}(\eta | Z) \bigg] + \text{Var} \bigg(  \mathbb{E}[\eta | Z] \bigg) \\
%     & = \mathbb{E}\bigg[ \mathbb{E}(\eta^2 | Z) - (\mathbb{E}(\eta | Z))^2 \bigg] + \text{Var} \bigg(  \mathbb{E}[\eta | Z] \bigg) \\
%     & = \mathbb{E}[\sigma^2_\eta - 0] + 0 \\
%     & = \sigma^2_\eta,
% \end{align*}
% where the first equality is due to the Law of Total Variance.
% The third equality uses the assumption that $\mathbb{E}(\eta^2 | Z) = \sigma^2_\eta$.

% Therefore, we have%
% \footnote{
% I use the fact that $P_Z$ is idempotent and $\text{Var}(\eta) = \sigma^2_\eta$ in the third equality.
% }
% \begin{align*}
% \text{Var}[\sqrt{n} (\hat{\beta}_{\text{OLS}} - \beta_0)] & = n \text{Var}[(X' P_Z X)^{-1}X' P_Z \eta] \\ 
% & = n (X' P_Z X)^{-1}X' P_Z \text{Var}(\eta) P_Z X (X' P_Z X)^{-1} \\ 
% & = n \sigma^2_\eta (X' P_Z X)^{-1}X' P_Z X (X' P_Z X)^{-1} \\ 
% & = n \sigma^2_\eta (X' P_Z X)^{-1}
% \end{align*}

\subsection{(f) Answer}
\noindent Due to conditional heteroskedasticity, $\sigma_0^2(Z) = \sigma_0^2$ and 
$\sigma_\eta^2(Z) = \sigma_\eta^2$ are functions of $Z$ and thus they do not hold in general.

Therefore, I propose the following heteroskedasticity-robust estimator for the asymptotic variance of 
$\sqrt{n}(\hat{\beta}_{\text{GMM}} - \beta_0)$:
\begin{align}
  \widehat{\Sigma} & = \bigg[\hat{G}'\widehat{W} \hat{G}\bigg]^{-1}, 
\end{align}
where $\widehat{W} = \left(\frac{1}{n} \sum_{i=1}^{n} Z_i Z_i' (Y_i - X_i' \hat{\beta}_{\text{2SLS}})^2 \right)^{-1}$.
$\hat{\beta}_{\text{2SLS}} = \left( X'P_Z X \right)^{-1} X'P_Z Y$.
$\hat{G} = \bm{Z}'\bm{X}/n$.

\noindent \textbf{Justifications for my proposals:}
\begin{itemize}
  \item First, the 2SLS estimator is efficient because $\mathbb{E}[Z\epsilon] = 0$. Namely,
  \[
  \mathbb{E}\bigg[\hat{\beta}_{\text{2SLS}}\bigg] = 
  \mathbb{E}\bigg[\left( X'P_Z X \right)^{-1} X'P_Z (X\beta_0 + \epsilon) \bigg] = \beta_0.
  \]
  \item Second, since $\mathbb{E}[Y_i - X_i' \hat{\beta}_{\text{2SLS}}] = \epsilon_i$, we have
  \[
    \left(\widehat{W}\right)^{-1} = \frac{1}{n} \sum_{i=1}^{n} Z_i Z_i' (Y_i - X_i' \hat{\beta}_{\text{2SLS}})^2 
    \rightarrow_p \mathbb{E}[Z Z' \epsilon^2] = \Omega.
  \]
  \item Finally, since $\hat{G} \rightarrow_p \mathbb{E}[ZX']$, we can conclude that $\widehat{\Sigma}$ is consistent.
\end{itemize}


%% =========== %% ========= %%
\bibliography{../bib/notes.bib}

\end{document}