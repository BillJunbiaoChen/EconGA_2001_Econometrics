\documentclass[11pt]{article}

%%METADATA
\title{GA2001 Econometrics (Part 2) \\Solution to Problem Set 4}
\author{
Junbiao Chen\thanks{E-mail: jc14076@nyu.edu.}
}
\date{\today}


%%PACKAGES
\usepackage{mdframed} % For boxed environments
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx}
\usepackage{setspace}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[hyphens]{url}
\usepackage{natbib}
\usepackage[font=normalsize,labelfont=bf]{caption}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue,citecolor=blue}
\usepackage{stmaryrd}  %Package with \boxast command
\usepackage{enumerate}% http://ctan.org/pkg/enumerate %Supports lowercase Roman-letter enumeration
\usepackage{verbatim} %Package with \begin{comment} environment
%\usepackage{enumitem}
\usepackage{physics}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{upquote}
\usepackage{booktabs} %Package with \toprule and \bottomrule
\usepackage{etoc}     %Package with \localtableofcontents
\usepackage{placeins}    %Package that prevent repositioning the tables
\usepackage{multicol}
\usepackage{bm}
\usepackage{subfig}
\usepackage{csquotes}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{language=bash,
  frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=false,
  tabsize=3
}

\lstset{language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=false,
  tabsize=4
}

% Define Julia style for lstlisting
\lstdefinelanguage{Julia}{
    morekeywords={
        for, end, in, if, else, elseif, while, break, continue, return, function,
        struct, mutable, begin, do, using, import, export, const, let, local, global,
        try, catch, finally, true, false, nothing, quote, macro, module, baremodule,
        where, abstract, typealias, type, bitstype
    },
    sensitive=true,
    morecomment=[l]{\#},
    morestring=[b]",
    morestring=[b]',
}

\lstset{
    language=Julia,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    breakatwhitespace=true,
    frame=single,
    captionpos=b
}


\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9} %

%CUSTOM DEFINITIONS
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\setcounter{secnumdepth}{3}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{automata, positioning, arrows, calc}

\tikzset{
	->,  % makes the edges directed
	>=stealth, % makes the arrow heads bold
	shorten >=2pt, shorten <=2pt, % shorten the arrow
	node distance=3cm, % specifies the minimum distance between two nodes. Change if n
	every state/.style={draw=blue!55,very thick,fill=blue!20}, % sets the properties for each ’state’ n
	initial text=$ $, % sets the text that appears on the start arrow
 }

%% PROPOSITION
% Define the Proposition environment
\newmdenv[
  innerleftmargin=10pt, 
  innerrightmargin=10pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  linecolor=black, 
  linewidth=1pt,
  backgroundcolor=white, 
  roundcorner=5pt
]{propositionbox}

\newtheoremstyle{boldtitle} % Define a new theorem style
  {10pt} % Space above
  {10pt} % Space below
  {\itshape} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  { } % Space after theorem head
  {} % Theorem head spec

\theoremstyle{boldtitle} % Use the custom style
\newtheorem{proposition}{Proposition} % Define the proposition environment

% Redefine the proposition environment to use the box
\newenvironment{boxedproposition}[1][]
{\begin{propositionbox}\begin{proposition}[#1]}
{\end{proposition}\end{propositionbox}}

%%FORMATTING
\usepackage[bottom]{footmisc}
\onehalfspacing
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\bibliographystyle{../bib/aeanobold-oxford}


%main text
\begin{document}
\maketitle
\section{Problem 1. Hausman-Wu Test}
\subsection{a. Answer}
To show $P_Z P_{Z_1} = P_{Z_1}$, note that 
\[
Z_1 = Z \begin{bmatrix}
    I_{k1} \\ 0
\end{bmatrix}
\]
Hence, we have 
\begin{align*}
Z(Z'Z)^{-1}Z' Z_1(Z_1'Z_1)^{-1}Z_1' & = Z(Z'Z)^{-1} Z' Z \begin{bmatrix}
    I_{k1} \\ 0
\end{bmatrix} (Z_1'Z_1)^{-1}Z_1' \\ 
& = Z \begin{bmatrix}
    I_{k1} \\ 0
\end{bmatrix}(Z_1'Z_1)^{-1}Z_1'\\
& = Z_1(Z_1'Z_1)Z_1'
\end{align*}

\subsection{b and c. Answers}
Using CLT, we have 
\[
\sqrt{n}\left(\begin{bmatrix} \hat{\beta}_1 \\ \hat{\beta}_2 \end{bmatrix} 
- \begin{bmatrix} \beta  \\ \beta \end{bmatrix}  \right)
\stackrel{d}{\rightarrow} \mathcal{N}(0, V)
\]
where $V$ is given by 
\[
V_{11}
= \sigma_0^{2}\left(
\mathbb{E}\!\left[x_i z_{i1}'\right]\,
\mathbb{E}\!\left[z_{i1} z_{i1}'\right]^{-1}\,
\mathbb{E}\!\left[z_{i1} x_i'\right]
\right)^{-1}
\]
\[
V_{22}
= \sigma_0^{2}\left(
\mathbb{E}\!\left[x_i z_i'\right]\,
\mathbb{E}\!\left[z_i z_i'\right]^{-1}\,
\mathbb{E}\!\left[z_i x_i'\right]
\right)^{-1}
\]
\[
V_{12} = 
\sigma_0^{2}\left(
\mathbb{E}\!\left[x_i z_i'\right]\,
\mathbb{E}\!\left[z_i z_i'\right]^{-1}\,
\mathbb{E}\!\left[z_i x_i'\right]
\right)^{-1}
\]
\[
V_{21} = V_{12}' = V_{12}.
\]
Hence we have 
\[
V_{22} - V_{12} - V_{21} = -V_{22}.
\]

\subsection{d. Answer}
Applying Delta method to the result above, we have 
\[
\sqrt{n} (\hat{\beta}_1 - \hat{\beta}_2) \stackrel{d}{\rightarrow} 
\mathcal{N} \left(0,\ V_{11}+V_{22}-V_{12}-V_{21}\right) 
= 
\mathcal{N}(0, V_{11} - V_{22}).
\]
\textbf{Remark:} Under the null hypothesis that both estimators are consistent 
and under regularity conditions ensuring a joint CLT,
the Hausman-Wu statistic follows $\chi^2$ distribution.
\[
H \;=\; n(\hat{\beta}_1-\hat{\beta}_2)'\,(V_{11}-V_{22})^{-1}\,(\hat{\beta}_1-\hat{\beta}_2)
\ \xrightarrow{d}\ \chi^2_{k},
\]
where $k=\dim(\beta)$.


\section{Problem 2. Abadie (2003)}
\subsection{a. solution}
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{$Z_i$} \\
\hline
 & $D_i(1) = 0$ & $D_i(1) = 1$ \\
\hline
$D_i(0) = 0$ & never takers & compliers \\
\hline
$D_i(0) = 1$ & defiers & always takers \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{b. solution}
By the law of total expectation, partitioning by the four latent compliance types $(D_{0i}, D_{1i})$:
$$
\mathbb{E}[X_{i}] = \sum_{l \in \{0, 1\}} \sum_{m \in \{0, 1\}} \mathbb{E}[X_{i}|D_{0i}=l, D_{1i}=m] \cdot P(D_{0i}=l, D_{1i}=m)
$$
Using the notation $h_{lm}$ and $\pi_{lm}$:
$$
\mathbb{E}[X_{i}] = \sum_{l,m} \pi_{lm} \cdot h_{lm}
$$
Expanding the sum:
$$
\mathbb{E}[X_{i}] = h_{00}\pi_{00} + h_{01}\pi_{01} + h_{10}\pi_{10} + h_{11}\pi_{11}
$$

\subsection{c. Solution}
Show that $\mathbb{E}[D_{i}(1-Z_{i})X_{i}]=\mathbb{E}[X_{i}|D_{i}=1,Z_{i}=0]P(D_{i}=1|Z_{i}=0)P(Z_{i}=0)$.

The term $D_{i}(1-Z_{i})X_{i}$ is non-zero only when $D_i=1$ and $Z_i=0$.
Using the law of total expectation conditioned on $D_i$ and $Z_i$:
\begin{align*}
\mathbb{E}[D_{i}(1-Z_{i})X_{i}] &= \mathbb{E}[D_{i}(1-Z_{i})X_{i} | D_i=1, Z_i=0] \cdot P(D_i=1, Z_i=0) \\
&= \mathbb{E}[1 \cdot (1-0) X_{i} | D_i=1, Z_i=0] \cdot P(D_i=1, Z_i=0) \\
&= \mathbb{E}[X_{i} | D_i=1, Z_i=0] \cdot P(D_{i}=1|Z_{i}=0) \cdot P(Z_{i}=0)
\end{align*}

\subsection{d. Solution}
From part (c):
$$
\mathbb{E}[D_{i}(1-Z_{i})X_{i}] = \mathbb{E}[X_{i} | D_i=1, Z_i=0] \cdot P(D_{i}=1|Z_{i}=0) \cdot P(Z_{i}=0)
$$
When $Z_i=0$, $D_i = D_{0i}$.
\begin{enumerate}
    \item $P(D_{i}=1|Z_{i}=0) = P(D_{0i}=1|Z_{i}=0)$. By independence, this is $P(D_{0i}=1)$.
    Under monotonicity ($\pi_{10}=0$), $P(D_{0i}=1) = P(D_{0i}=1, D_{1i}=1) = \pi_{11}$.
    \item $\mathbb{E}[X_{i} | D_i=1, Z_i=0] = \mathbb{E}[X_{i} | D_{0i}=1, Z_i=0]$. By independence and monotonicity, $D_{0i}=1$ implies membership in the Always-Taker group ($D_{1i}=1$). Thus, $\mathbb{E}[X_{i} | D_{0i}=1, Z_i=0] = \mathbb{E}[X_{i} | D_{0i}=1, D_{1i}=1] = h_{11}$.
\end{enumerate}
Substituting these results:
$$
\mathbb{E}[D_{i}(1-Z_{i})X_{i}] = \mathbb{E}[X_{i}|D_{0i}=1,D_{1i}=1] \cdot P(D_{0i}=1,D_{1i}=1) \cdot P(Z_{i}=0)
$$

\subsection{e. Solution}
When $Z_i=1$, $D_i = D_{1i}$.
\begin{enumerate}
    \item $P(D_{i}=0|Z_{i}=1) = P(D_{1i}=0|Z_{i}=1)$. By independence, this is $P(D_{1i}=0)$.
    Under monotonicity ($\pi_{10}=0$), $P(D_{1i}=0) = P(D_{0i}=0, D_{1i}=0) = \pi_{00}$.
    \item $\mathbb{E}[X_{i} | D_i=0, Z_i=1] = \mathbb{E}[X_{i} | D_{1i}=0, Z_i=1]$. By independence and monotonicity, $D_{1i}=0$ implies membership in the Never-Taker group ($D_{0i}=0$). Thus, $\mathbb{E}[X_{i} | D_{1i}=0, Z_i=1] = \mathbb{E}[X_{i} | D_{0i}=0, D_{1i}=0] = h_{00}$.
\end{enumerate}
Substituting these results:
$$
\mathbb{E}[(1-D_{i})Z_{i}X_{i}] = \mathbb{E}[X_{i}|D_{0i}=0,D_{1i}=0] \cdot P(D_{0i}=0,D_{1i}=0) \cdot P(Z_{i}=1)
$$

\subsection{f. solution}
First, we can express $\mathbb{E}[w_{i}X_{i}]$ as follows:
$$
\mathbb{E}[w_{i}X_{i}] = \mathbb{E}[X_{i}] - \frac{\mathbb{E}[D_{i}(1-Z_{i})X_{i}]}{P(Z_{i}=0)} - \frac{\mathbb{E}[(1-D_{i})Z_{i}X_{i}]}{P(Z_{i}=1)}
$$
Substitute the simplified expressions from (d) and (e):
\begin{align*}
\mathbb{E}[w_{i}X_{i}] &= \mathbb{E}[X_{i}] - \mathbb{E}[X_{i}|D_{0i}=1,D_{1i}=1]P(D_{0i}=1,D_{1i}=1) \\
& \quad - \mathbb{E}[X_{i}|D_{0i}=0,D_{1i}=0]P(D_{0i}=0,D_{1i}=0)
\end{align*}
Using $h_{lm}$ and $\pi_{lm}$ notation:
$$
\mathbb{E}[w_{i}X_{i}] = \mathbb{E}[X_{i}] - h_{11}\pi_{11} - h_{00}\pi_{00}
$$
Substitute $\mathbb{E}[X_{i}] = h_{00}\pi_{00} + h_{01}\pi_{01} + h_{10}\pi_{10} + h_{11}\pi_{11}$ from part (b):
\begin{align*}
\mathbb{E}[w_{i}X_{i}] &= (h_{00}\pi_{00} + h_{01}\pi_{01} + h_{10}\pi_{10} + h_{11}\pi_{11}) - h_{11}\pi_{11} - h_{00}\pi_{00} \\
&= h_{01}\pi_{01} + h_{10}\pi_{10}
\end{align*}
Under monotonicity, $\pi_{10}=P(D_{0i}=1, D_{1i}=0)=0$.
$$
\mathbb{E}[w_{i}X_{i}] = h_{01}\pi_{01} = \mathbb{E}[X_{i}|D_{0i}=0,D_{1i}=1]P(D_{0i}=0,D_{1i}=1)
$$

% \paragraph{Usefulness:}
% The group $(D_{0i}=0, D_{1i}=1)$ is the **Complier** group. The result shows that $\mathbb{E}[w_{i}X_{i}]$ is the expected value of $X_i$ among compliers, weighted by the probability of being a complier.

% The mean characteristic of the complier group, $\mathbb{E}[X_{i}|\text{Compliers}]$, can be estimated using the sample analogue of the ratio:
% $$
% \mathbb{E}[X_{i}|\text{Compliers}] = \frac{\mathbb{E}[w_{i}X_{i}]}{\mathbb{E}[w_{i}]}
% $$
% where $\mathbb{E}[w_{i}] = P(D_{0i}=0, D_{1i}=1)$, the probability of being a complier. This is useful because it allows the researcher to characterize the observed characteristics ($X_i$) of the complier group, which is the subset of individuals for whom the 2SLS estimator (LATE) identifies the treatment effect.


\section{Problem 3. Weak and Many Instrumental Variables}
My code is available at 

\texttt{
  https://github.com/BillJunbiaoChen/EconGA\_2001\_Econometrics/Part2/ProblemSets/weak\_iv.
}
\subsection{a. Answer}
We can see that adding junk instruments contaminates the slope estimate of 2SLS.
However, if we have large number of observations, this contamination is not a big issue.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{weak_iv/images/comparison_ols_iv.png}
  \caption{Slope: OLS vs. IV under weak/many intruments}
\end{figure}


\subsection{b. Answer}
See Figure~3.2.

\begin{figure}[htbp]
  \label{fig:many_iv_se}
  \centering
  \includegraphics[width=0.9\textwidth]{weak_iv/images/mean_2sls_se_by_nK.png}
  \caption{Standard Error: IV under weak/many intruments}
\end{figure}


% \subsection{c. Answer}


\section{GLS vs. FE Estimator}
\subsection{a. Answer}
By the definition of $\dot{X}$, $\bar{X}$, $\dot{y}$, and $\bar{y}$, we have 
\[
X_* = \dot{X} + \lambda \bar{X}, \quad \text{ and } \quad y_* = \dot{y} + \lambda \bar{y}
\]
Hence, we have 
\begin{align*}
\hat{\beta}_{GLS} & = \left[X_*'X_* \right]^{-1}X_*' y_* \\
& = \left[\dot{X}'\dot{X} + \lambda^2 T  \bar{X}' \bar{X} \right]^{-1} 
\left( \dot{X}'\dot{y} + \lambda^2 T  \bar{X}' \bar{y}  \right).
\end{align*}

\subsection{b. Answer}
Observe that 
$\hat{\beta}_{GLS} - \hat{\beta}_{w} = R \left(\hat{\beta}_{FE} - \hat{\beta}_{w}\right)$.
Hence, we have 
\begin{align*}
R & = \left(\hat{\beta}_{GLS} - \hat{\beta}_{w}  \right) \left(\hat{\beta}_{FE} - \hat{\beta}_{w}\right)^{-1} \\ 
& = \left(I + T \lambda^2 \left(\bar{X}'\bar{X} \right)^{-1}\bar{X}'\bar{X} \right)^{-1}
\end{align*}


\subsection{c. Answer}
As $T \rightarrow \infty$, the relative weight shrinks to zero.
It follows that $\hat{\beta}_{GLS} \rightarrow \hat{\beta}_{FE}$.

\subsection{d. Answer}
Denoting $\otimes$ the kronecker product, we can derive 
\begin{align*}
\mathbb{E}(\hat{\beta}_{FE}) & = 
\mathbb{E}\left[\left(X' (I_n \otimes M_T) X \right)^{-1} \left(X' (I_n \otimes M_T) y \right)\right] \\ 
& = \beta + \mathbb{E}\left[\left(X' (I_n \otimes M_T) X \right)^{-1} \left(X' (I_n \otimes M_T) (\alpha \otimes 1_T) \right)\right] \\
& = \beta
\end{align*}

as well as
\begin{align*}
\mathbb{E}(\hat{\beta}_{W}) & = 
\mathbb{E}\left[\left(\bar{X}' \bar{X} \right)^{-1} \left(\bar{X}' \bar{y} \right)\right] \\ 
& = \beta + \mathbb{E}\left[\left(\bar{X}' \bar{X} \right)^{-1} \bar{X}' \alpha \right]
\end{align*}

Hence, $\hat{\beta}_{FE}$ is unbiased if the individual FE is correlated with $X$, while 
the witin-estimator is biased (similar to an OVB). 
The magnitude of bias is $\mathbb{E}\left[\left(\bar{X}' \bar{X} \right)^{-1} \bar{X}' \alpha \right]$.


\subsection{d. Answer}
Since $\hat{\beta}_{GLS} = R\hat{\beta}_{FE} + (1 - R)\hat{\beta}_{W}$, we have 
\[
\text{Bias}(\hat{\beta}_{GLS}) = (1 - R) \text{Bias}(\hat{\beta}_{W}) = (1 - R) \mathbb{E}\left[\left(\bar{X}' \bar{X} \right)^{-1} \bar{X}' \alpha \right].
\]
As $\hat{\beta}_{GLS} \rightarrow \hat{\beta}_{FE}$ as $T \rightarrow \infty$, 
the bias of the GLS estimator shrinks to zero as $T \rightarrow \infty$.



\section{Fixed Effects and Measurement Error}
\subsection{a. Solution}
Since the model specification follows a classical measurement error model, we have 
\[
\Delta x_{i,t} = (\rho - 1) x_{i,t-1}^* + \xi_{it} + \Delta v_{i,t}
\]
\[
\Delta y_{i,t} = \beta_1 (\rho - 1) x_{i,t-1}^* + \beta_1 \xi_{it} + \Delta v_{i,t} + \Delta e_{i,t}
\]
Hence, the probability limit of $\hat{\beta}_{FD}$ is 
\begin{align*}
\text{plim}(\hat{\beta}_{FD}) & = \frac{\text{Cov}(\Delta x_{i,t}, \Delta y_{i,t})}{\text{Var}(\Delta x_{i,t})} \\
& = \frac{\beta (\rho - 1)^2 \text{Cov}(\Delta x_{i,t-1}^*) + \beta_1 \sigma_\xi^2}{(\rho - 1)^2 \text{Cov}(\Delta x_{i,t-1}^*) + \sigma_\xi^2 + 2 \sigma_v^2}
\end{align*}


\subsection{b. Solution}
Define $\Delta^2 x_{it} = x_{it} - x_{it-2}$.
Following similar derivation above, we have 
\begin{align*}
\text{plim}(\hat{\beta}_{SD}) & = \frac{\text{Cov}(\Delta^2 x_{i,t}, \Delta^2 y_{i,t})}{\text{Var}(\Delta^2 x_{i,t})} \\
& = \frac{\beta (\rho - 1)^2 \text{Cov}(\Delta x_{i,t-2}^*) + \beta_1 \sigma_\xi^2 + \beta_1 \rho^2 \sigma_\xi^2
}{(\rho^2 - 1)^2 \text{Cov}(\Delta x_{i,t-1}^*) + \sigma_\xi^2 + \rho^2 \sigma_\xi^2 +  2 \sigma_v^2}
\end{align*}

\subsection{c. Solution}
Since
\begin{align*}
\overline{y}_{i} & = \frac{1}{T}\sum_{t=1}^{T}y_{it}=\frac{1}{T}\sum_{t=1}^{T}(\beta_{1}x_{it}^{*}+\alpha_{i}+e_{it}) \\ 
& = \beta_{1}\overline{x_{i}}^{*}+\alpha_{i}+\overline{e}_{i}
\end{align*}
and

$$
y_{it}-\overline{y}_{i}=\beta_{1}(x_{it}^{*}-\overline{x_{i}}^{*})+(e_{it}-\overline{e}_{i})
$$

$$
\overline{x}_{i}=\frac{1}{T}\sum_{t=1}^{T}x_{it}=\frac{1}{T}\sum_{t=1}^{T}(x_{it}^{*}+v_{it})=\overline{x_{i}}^{*}+\overline{v}_{i}
$$

and
$$
x_{it}-\overline{x}_{i}=(x_{it}^{*}-\overline{x_{i}}^{*})+(v_{it}-\overline{v}_{i})
$$

Assume Stationarity on the AR(1) process.
$$
Var(X_{it}^{*})=Var(\rho X_{it-1}^{*}+\xi_{it})=\rho^{2}Var(X_{it-1}^{*})+\sigma_{\xi}^{2}
$$

So $Var(X_{it}^{*})\equiv\sigma_{*}^{2}=\frac{\sigma_{\xi}^{2}}{1-\rho^{2}}$

\begin{align*}
  Var(x_{it}^{*}-\overline{x}_{i}^{*}) & = Var(x_{it}^{*})-2Cov(x_{it}^{*},\overline{x}_{i}^{*})+Var(\overline{x}_{i}^{*}) \\
& = \frac{\sigma_{\xi}^{2}}{1-\rho^{2}}+\frac{1}{T}\cdot\frac{\sigma_{\xi}^{2}}{1-\rho^{2}}-2Cov(x_{it}^{*},\overline{x}_{i}^{*}) \\ 
& =\frac{\sigma_{\xi}^{2}}{1-\rho^{2}}+\frac{1}{T}\cdot\frac{\sigma_{\xi}^{2}}{1-\rho^{2}}-2\cdot\frac{1}{T}\cdot\frac{\sigma_{\xi}^{2}}{1-\rho^{2}} \\
& = \frac{T-1}{T}\cdot\frac{\sigma_{\xi}^{2}}{1-\rho^{2}}
\end{align*}
The probability limit is:
$$
\frac{Cov(y_{it}-\overline{y}_{i},x_{it}-\overline{x}_{i})}{Var(x_{it}-\overline{x}_{i})}=\frac{\beta_{1}\cdot\frac{T-1}{T}Var(x_{it}^{*})}{\frac{T-1}{T}\cdot Var(x_{it}^{*})+\frac{T-1}{T}\sigma_{v}^{2}}
$$

\subsection{e. Solution}
$$
\frac{\hat{\beta}_{W}}{\hat{\beta}_{FD}}\rightarrow1+\frac{\rho\sigma_{v}^{2}}{\sigma_{\xi}^{2}+\sigma_{v}^{2}}\Rightarrow\frac{\hat{\beta}_{W}-\hat{\beta}_{FD}}{\hat{\beta}_{FD}}\rightarrow\frac{\rho\sigma_{v}^{2}}{\sigma_{\xi}^{2}+\sigma_{v}^{2}}
$$

Then
By using the estimate of $\rho$, denoted $\hat{\rho}$,

$$
\frac{\hat{\beta}_{W}-\hat{\beta}_{FD}}{\hat{\rho}\cdot\hat{\beta}_{FD}}\rightarrow\frac{\sigma_{v}^{2}}{\sigma_{\xi}^{2}+\sigma_{v}^{2}}
$$

Hence, the corrected estimator is:
$$
\frac{\hat{\rho}\hat{\beta}_{FD}}{(\hat{\rho}+1)\hat{\beta}_{FD}-\hat{\beta}_{W}}
$$

\subsection{f. Solution}
Note that
\begin{align*}
  x_{it}-x_{it-1} & = x_{it}^{*}-x_{it-1}^{*}+v_{it}-v_{it-1} \\ 
  & = (\rho-1)x_{it-1}^{*}+\xi_{it}+v_{it}-v_{it-1}
\end{align*}
In addition,
\begin{align*}
y_{it}-y_{it-1} & = \beta_{1}(x_{it}^{*}-x_{it-1}^{*})+e_{it}-e_{it-1} \\
& = \beta_{1}(x_{it}-x_{it-1})-\beta_{1}(v_{it}-v_{it-1})+e_{it}-e_{it-1}
\end{align*}

\begin{align*}
Cov(y_{it}-y_{it-1},x_{it-2}) & = 
Cov((\rho-1)x_{it-1}^{*}+\xi_{it}+v_{it}-v_{it-1}, x_{it-2}^{*}+v_{it-2}) \\
& = Cov(\rho x_{it-2}^{*}+\xi_{it-1}(\rho-1)+\xi_{it}+v_{it}-v_{it-1}, x_{it-2}^{*}+v_{it-2}) \\
& = 0
\end{align*}
which satisfies the independence requirement.

Thus,
$x_{it-2}$ can be an instrument for FD model.

%% =========== %% ========= %%
\bibliography{../bib/notes.bib}

\end{document}